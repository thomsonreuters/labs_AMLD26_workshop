## Break the model, Build the trust: A Practical Blueprint for securing LLMs with PromptFoo

**Speakers:** Gianfranco Romani, Andreea Iuga, Simone Clemente

### Description

Large Language Models are everywhere, but are they secure? In this hands-on workshop, you'll go behind the scenes and learn the essentials of AI security testingâ€”the practice of ethically hacking a system to find and fix its weaknesses.
You will be working with a target model. Your goal is to uncover potential security flaws, from prompt injections to other unexpected behaviors, and learn how to prevent them.

**Target model:** today you will be working with a customer service chatbot, one of the most common real-world AI applications. While it's built to help people, you'll be trying to make it misbehave. Your goal is to find security vulnerabilities, execute prompt injection attacks, and see what other unintended actions you can force it to take.

#### PART 1: Manual testing

First, you'll get your hands dirty. Using a simple chat interface, you'll manually interact with the AI, trying to trick it, confuse it, and make it do things it shouldn't. This phase is all about learning to think creatively and develop an instinct for finding vulnerabilities.

You can find the detailed instructions to run the chat UI [here](./instructions/1_chat_interface.md).

#### PART 2: Automated testing with PromptFoo

Next, you'll discover why manual testing isn't enough for real-world applications. We'll introduce you to PromptFoo, an automated tool used by professionals. You'll learn how to configure it to run thousands of security tests, saving you time and dramatically expanding your ability to find flaws. PromptFoo does the heavy lifting so you can focus on strategy.

[Here](./instructions/2_promptfoo_guide.md) you can find all the instructions to install and run PromptFoo on your device.

### Useful material
